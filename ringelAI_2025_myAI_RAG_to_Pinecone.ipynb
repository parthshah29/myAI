{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **myAI: Populating the Pinecone RAG Database**\n","\n","Daniel M. Ringel  \n","Kenan-Flagler Business School  \n","*The University of North Carolina at Chapel Hill*  \n","dmr@unc.edu\n","\n","### ***Adding Documents to PineCone without RAGLoader***\n","\n","***Note***: We depricated RAGLoader after I completed my teaching at KFBS in the Spring 2025 Semester.\n","\n","[myAI on GitHub](https://github.com/dringel/myAI)\n","\n","*April 28, 2025*  \n","Version 1.0"],"metadata":{"id":"pgZBYZdXm1ws"}},{"cell_type":"markdown","source":["# From Raw Content to Pinecone"],"metadata":{"id":"Q8i2bcWJOXcj"}},{"cell_type":"markdown","source":["\n","This notebook walks through a simple, end-to-end process for indexing unstructured text into Pinecone using semantic chunking. It starts by installing the required Python packages, then creates a Pinecone index tailored to a specific embedding model. Raw documents are split into smaller, semantically meaningful chunks, which are then upserted into the index for vector search. The final step demonstrates how to query the indexed content using natural language.\n","\n","**WARNING**: myAI ([myAI on GitHub](https://github.com/dringel/myAI)) was originally set-up to work with OpenAI Embeddings. If you do not change anything on your github and vercel delpoyment, then you need to use them (see bottom of notebook) to insert documents (chuncks) into Pinecone.\n","This means that myAI originally used OpenAI embeddings directly, without relying on Pinecone's built-in embedding integration.\n","\n","**IMPORTANT**: The myAI repo on GitHub was updated in April 2025 so that you can decide between using OpenAI Embeddings and internal Pinecone Embeddings. Your chatbot will be faster when you use internal Pinecone embeddings, but you may have some volume constraints on a free plan and you are locked-in to Pinecone.\n","\n","> #### **Please note that you cannot use the two in Tandem**, or your upserting process will fail. If you wish to use OpenAI embeddings, please skip to the end after installing the prerequisites.\n","\n","### To start, upload your secret keys using Google Secrets on your left.\n","\n","![Secrets](https://mapxp.app/BUSI488/secret-collab-pinecone-s.png)\n","\n"],"metadata":{"id":"UXFNKc8M3lIk"}},{"cell_type":"markdown","source":["## ***Prerequisites***\n","\n","You need to install a few packages / libraries to start."],"metadata":{"id":"o0HTdgpuOxtg"}},{"cell_type":"code","source":["# Running these installations should take a couple of minutes\n","\n","%pip install pinecone # for creating and managing vector indexes\n","%pip install unstructured['all-docs'] # for parsing various document formats (PDFs, Word, HTML, etc.) and chunking\n","%pip install openai # for generating embeddings if not using Pinecone‚Äôs built-in models\n","%pip install pdf2image pytesseract"],"metadata":{"id":"IKy7EA3RPAQe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 1**: Using Unstructured and Ensuring Correct Setups"],"metadata":{"id":"keNPFTaB6Qk_"}},{"cell_type":"markdown","source":["What is ‚Äé`unstructured`?\n","\n","The ‚Äé`unstructured` Python library provides flexible tools for parsing and extracting text from a wide range of document types. It supports both structured and unstructured content, and is designed to work with PDFs, Word documents, HTML, emails, spreadsheets, and more. It is particularly useful for preparing raw content for downstream tasks such as chunking which we will use later.\n","\n","At its core, ‚Äé`unstructured` uses a modular pipeline to ‚Äúpartition‚Äù documents into clean, structured elements like titles, paragraphs, tables, and images. This makes it easy to convert complex files into plain text or structured formats suitable for LLM and vector workflows. The next code block shall test that you have correctly installed packages."],"metadata":{"id":"brUGAc5J9XbV"}},{"cell_type":"markdown","source":["As per the documentation, unstructured supports the following file types, if you want to change to a specific partitioning function, refer to the documentation [here](https://docs.unstructured.io/open-source/core-functionality/partitioning). If you wish to upload an image, please just upload a text describing the image and include the link of the image as the source url later.\n","\n","üìÑ Text Documents\n"," ‚Ä¢ Plain Text (‚Äé‚Å†.txt‚Å†, ‚Äé‚Å†.text‚Å†, ‚Äé‚Å†.log‚Å†)\n"," ‚Ä¢ Markdown (‚Äé‚Å†.md‚Å†)\n"," ‚Ä¢ ReStructuredText (‚Äé‚Å†.rst‚Å†)\n"," ‚Ä¢ Rich Text Format (‚Äé‚Å†.rtf‚Å†)\n"," ‚Ä¢ Org Mode (‚Äé‚Å†.org‚Å†)\n"," ‚Ä¢ XML (‚Äé‚Å†.xml‚Å†)\n","\n","üìù Word Processing\n"," ‚Ä¢ Microsoft Word (‚Äé‚Å†.doc‚Å†, ‚Äé‚Å†.docx‚Å†)\n"," ‚Ä¢ OpenOffice (‚Äé‚Å†.odt‚Å†)\n"," ‚Ä¢ EPUB (‚Äé‚Å†.epub‚Å†)\n","\n","üìä Spreadsheets & Tables\n"," ‚Ä¢ Excel (‚Äé‚Å†.xlsx‚Å†, ‚Äé‚Å†.xls‚Å†)\n"," ‚Ä¢ CSV (‚Äé‚Å†.csv‚Å†)\n"," ‚Ä¢ TSV (‚Äé‚Å†.tsv‚Å†)\n","\n","üìß Emails\n"," ‚Ä¢ EML (‚Äé‚Å†.eml‚Å†)\n"," ‚Ä¢ MSG (‚Äé‚Å†.msg‚Å†)\n","\n","üåê Web & Code\n"," ‚Ä¢ HTML (‚Äé‚Å†.html‚Å†, ‚Äé‚Å†.htm‚Å†)\n"," ‚Ä¢ Code Files (‚Äé‚Å†.js‚Å†, ‚Äé‚Å†.py‚Å†, ‚Äé‚Å†.java‚Å†, ‚Äé‚Å†.cpp‚Å†, ‚Äé‚Å†.cc‚Å†, ‚Äé‚Å†.cxx‚Å†, ‚Äé‚Å†.c‚Å†, ‚Äé‚Å†.cs‚Å†, ‚Äé‚Å†.php‚Å†, ‚Äé‚Å†.rb‚Å†, ‚Äé‚Å†.swift‚Å†, ‚Äé‚Å†.ts‚Å†, ‚Äé‚Å†.go‚Å†)\n","\n","üìΩÔ∏è Presentations\n"," ‚Ä¢ PowerPoint (‚Äé‚Å†.ppt‚Å†, ‚Äé‚Å†.pptx‚Å†)\n","\n","üì∑ Images\n"," ‚Ä¢ Image Files (‚Äé‚Å†.png‚Å†, ‚Äé‚Å†.jpg‚Å†, ‚Äé‚Å†.jpeg‚Å†, ‚Äé‚Å†.tiff‚Å†, ‚Äé‚Å†.bmp‚Å†, ‚Äé‚Å†.heic‚Å†)\n","\n","üìö PDFs\n"," ‚Ä¢ PDF (‚Äé‚Å†.pdf‚Å†)\n","\n"],"metadata":{"id":"GcMhlTWM-zKR"}},{"cell_type":"code","source":["from google.colab import files\n","from unstructured.partition.auto import partition\n","\n","uploaded = files.upload() # upload a file (PDF, DOCX, TXT, etc.)\n","\n","file_path: str  = list(uploaded.keys())[0] # pick the first file as a test\n","\n","elements = partition(filename=file_path)\n","\n","RAW_TEXT_EXAMPLE: str = \"\\n\\n\".join([str(el) for el in elements])\n","print(RAW_TEXT_EXAMPLE) # you should see the raw text of whatever file you just uploaded"],"metadata":{"id":"EzqTWfIW6VQC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*If you do not see your raw text from the document, try reinstalling the packages or restarting your session to get the latest versions running. If your code does work, congratulations - we can now move onto the next step!*"],"metadata":{"id":"XPbjRB_zArcj"}},{"cell_type":"markdown","source":["## **Step 2:** Create an Index"],"metadata":{"id":"NbE6yu-IYJYp"}},{"cell_type":"markdown","source":["What is a Pinecone Index? [Documentation](https://docs.pinecone.io/guides/indexes/create-an-index)\n","\n","A Pinecone index is a cloud-hosted vector database that stores and retrieves embeddings for fast and scalable similarity search. You can think of an index as a smart container for vectorized data (like text embeddings), where each record is stored with a unique ID and optional metadata.\n","\n","When you create an index, you define:\n"," ‚Ä¢ Name: A unique identifier for your index\n","\n"," ‚Ä¢ Cloud and region: Where the index is hosted (e.g., AWS, GCP)\n","\n"," ‚Ä¢ Embedding model: Either managed by Pinecone or external (like OpenAI)\n","\n"," ‚Ä¢ Field mapping: How your data maps to the embedding input (e.g., ‚Äé`\"text\": \"chunk_text\"`)\n","\n","Once your index is created, you can:\n","\n"," ‚Ä¢ Upsert: Insert or update records (vectors with IDs and metadata)\n","\n"," ‚Ä¢ Query: Search for similar records using a new input vector\n","\n"," ‚Ä¢ Delete: Remove records by ID\n","\n"," ‚Ä¢ Fetch: Retrieve records by ID\n","\n","We will only focus on querying and upserting vectors for our notebook. But first, lets create an index."],"metadata":{"id":"utoZ7l3CB-V9"}},{"cell_type":"code","source":["from pinecone import Pinecone\n","from google.colab import userdata\n","import os\n","\n","# Before running this code block, be sure to add your key value pairs (OpenAI/Pinecone) in the secret tab to the left, allowing access to the notebook\n","pc: str = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"))\n","\n","INDEX_NAME: str = input(\"Enter a name for your Pinecone Index: \")\n","\n","# the fields cloud, region, and embed are all customizable and you should refer to documentation if you wish to change things.\n","if not pc.has_index(INDEX_NAME):\n","    pc.create_index_for_model(\n","        name=INDEX_NAME,\n","        cloud=\"aws\",\n","        region=\"us-east-1\",\n","        embed={\n","            \"model\":\"multilingual-e5-large\",\n","            \"field_map\":{\"text\": \"chunk_text\"}\n","        }\n","    )"],"metadata":{"id":"-l6wUfeiPQ66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*A key thing to note when using your index is that you cannot switch between using other embedding models and integrated embeddings, so when you set up your index for the first time, you can only use that initial embedding process. For safety, try to set up new indexes using this notebook.*"],"metadata":{"id":"FczgTbvkD8WR"}},{"cell_type":"markdown","source":["## **Step 3:** Making Chunks from Documents"],"metadata":{"id":"ecZS0tX6QSgk"}},{"cell_type":"markdown","source":["Chunks are smaller sections of a larger document. Instead of sending an entire file to an embedding or vector database, we break it down into manageable pieces that preserve meaning. This is especially helpful when working with long-form content like PDFs, articles, or transcripts.\n","\n","Breaking text into chunks improves search accuracy and performance. Each chunk becomes a standalone unit for embedding, indexing, and querying. We clean chunks before upserting to improve meaning. When a user asks a question, the system searches across all the chunks to find the most relevant ones.\n","\n","In this notebook, we use the ‚Äé`unstructured` library to create smart chunks that align with natural language structure, not just fixed word or character counts. This helps each chunk retain context and stay semantically useful. In this section, we will chunk the actual files you want to use"],"metadata":{"id":"HAux_E0CE4fc"}},{"cell_type":"markdown","source":["*You can run the following two code cells as much as possible to insert as many records as you want into Pinecone. You do not need to rerun the whole notebook. However, you may only insert one file at a time.*"],"metadata":{"id":"NBZLQAiXIdYS"}},{"cell_type":"code","source":["from unstructured.chunking.basic import chunk_elements\n","from google.colab import files\n","from unstructured.partition.auto import partition\n","\n","uploaded = files.upload() # upload your file\n","\n","file_path: str = list(uploaded.keys())[0]\n","source_citation: str = input(\"Enter the source description: \") # can change this line just to assigning your metadata\n","source_url: str = input(\"Enter the url of the source: \")\n","\n","elements = partition(filename=file_path)\n","chunked_elements = chunk_elements(elements)\n","\n","chunks: list[str] = []\n","\n","for chunk in chunked_elements:\n","    chunks.append(chunk.text.replace(\"\\n\", \" \"))"],"metadata":{"id":"1S44KswyO0WP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 4:** Refining and Upserting Document Chunks to Pinecone\n"],"metadata":{"id":"iLKlGrWLSbIB"}},{"cell_type":"markdown","source":["What is Upserting?\n","\n","Upserting is the process of inserting new records - vectors and texts - into a Pinecone index, or updating them if they already exist. In this notebook, we take each chunk of text and prepare it with extra metadata before sending it to Pinecone.\n","\n","We start by wrapping the list of chunks with empty strings at the beginning and end. This lets us safely create pre and post context around each chunk‚Äîhelpful for improving search relevance later.\n","\n","Then we loop through the chunks and build a list of dictionaries, each representing a record to be indexed. Each record includes:\n","\n"," ‚Ä¢ a unique ‚Äé`id` (generated using ‚Äé`uuid`)\n","\n"," ‚Ä¢ the chunk text\n","\n"," ‚Ä¢ the order of the chunk\n","\n"," ‚Ä¢ the text that comes before and after the chunk\n","\n"," ‚Ä¢ source metadata (description and URL)\n","\n"," **This code block is using integrated embeddings, so if you plan to use another type of embedding model this code will not work.**\n"],"metadata":{"id":"HVMrw77kL3FX"}},{"cell_type":"code","source":["import uuid\n","\n","data = [\"\"] + chunks + [\"\"] # chunks from the previous cell\n","\n","data_to_upsert = []\n","\n","# create pre and post contexts with accurate metadata\n","for i in range(1, len(data) - 1):\n","    if len(data[i]) < 3:\n","        continue\n","    data_to_upsert += [{\"id\": str(uuid.uuid4()), \"text\": data[i], \"order\": i - 1, \"post_context\": data[i + 1], \"pre_context\": data[i - 1], \"source_description\": source_citation, \"source_url\": source_url}]\n","\n","refined_chunks = [data_to_upsert[i:min(i + 48, len(data_to_upsert))] for i in range(0, len(data_to_upsert), 48)]\n","\n","# upserting via batches\n","for chunk_batch in refined_chunks:\n","  formatted_batch = []\n","  for record in chunk_batch:\n","    formatted_batch.append({\n","            \"id\": record[\"id\"],\n","            \"chunk_text\": record[\"text\"],\n","            \"order\": record[\"order\"],\n","            \"pre_context\": record[\"pre_context\"],\n","            \"post_context\": record[\"post_context\"],\n","            \"source_description\": record[\"source_description\"],\n","            \"source_url\": record[\"source_url\"]\n","        })\n","    pc.Index(INDEX_NAME).upsert_records(INDEX_NAME, formatted_batch)\n","\n","\n"],"metadata":{"id":"pXn5VZ8aRQaT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 5:** Querying Records"],"metadata":{"id":"CgW8Zh0ZWL_Y"}},{"cell_type":"markdown","source":["What is Querying?\n","\n","Once your chunks are indexed in Pinecone, you can search them using natural language queries. This is done through Pinecone‚Äôs ‚Äé‚Å†search_records‚Å† method, which compares your query text to all stored vectors and returns the most relevant matches.\n","\n","In this notebook, we use the integrated embedding model to automatically convert your search text into a vector behind the scenes. You don‚Äôt need to generate the embedding yourself ‚Äî just pass in your input\n","\n","This call returns the top 2 chunks that are most semantically similar to your search. You can increase ‚Äé‚Å†top_k‚Å† to get more results.\n","\n","Each result includes the original chunk text, metadata (like source URL), and a similarity score. This lets you surface the most relevant information from your indexed documents in response to user questions."],"metadata":{"id":"PRDl4RBMNSvV"}},{"cell_type":"code","source":["search_text: str = input(\"Enter a search query: \")\n","index_name: str = input(\"Enter the name of the index you want to query: \")\n","\n","response = pc.Index(index_name).search_records(\n","    index_name,\n","    query= {\n","        \"inputs\": {\"text\": search_text},\n","        \"top_k\": 2\n","    }\n",")\n","\n","results: dict = response.to_dict()\n","\n","for match in results[\"result\"][\"hits\"]:\n","    print(match[\"fields\"][\"chunk_text\"])\n","\n","\n"],"metadata":{"id":"wEh7s-D4Sypi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Using OpenAI Embeddings**: as originally done with myAI before April 2025\n","\n","[myAI on GitHub](https://github.com/dringel/myAI)"],"metadata":{"id":"OpAkwNR2XAzV"}},{"cell_type":"markdown","source":["If you would instead choose to use OpenAI embeddings, you may chunk and embed your documents here. You can edit your model but be careful that it adheres to your Pinecone index dimensions.\n","\n","**WARNING**: myAI was originally set-up to work with OpenAI Embeddings. If you do not change anything on your github and vercel delpoyment, then you need to use this part here to insert documents (chuncks) into Pinecone."],"metadata":{"id":"kBEjlyxhfsgT"}},{"cell_type":"code","source":["from google.colab import files\n","from unstructured.partition.auto import partition\n","from google.colab import files\n","from unstructured.partition.auto import partition\n","from openai import OpenAI\n","from google.colab import userdata\n","from pinecone import Pinecone\n","\n","uploaded = files.upload() # upload your file\n","\n","file_path = list(uploaded.keys())[0]\n","source_citation = input(\"Enter the source description: \")\n","source_url = input(\"Enter the url of the source: \")\n","\n","elements = partition(filename=file_path)\n","chunked_elements = chunk_elements(elements)\n","\n","chunks: list[str] = []\n","\n","for chunk in chunked_elements:\n","    chunks.append(chunk.text.replace(\"\\n\", \" \"))\n","\n","\n","data = [\"\"] + chunks + [\"\"]\n","\n","data_to_upsert = []\n","\n","# create pre and post contexts with accurate metadata\n","for i in range(1, len(data) - 1):\n","    if len(data[i]) < 3:\n","        continue\n","    data_to_upsert += [{\"id\": str(uuid.uuid4()), \"text\": data[i], \"order\": i - 1, \"post_context\": data[i + 1], \"pre_context\": data[i - 1], \"source_description\": source_citation, \"source_url\": source_url}]\n","\n","refined_chunks = [data_to_upsert[i:min(i + 48, len(data_to_upsert))] for i in range(0, len(data_to_upsert), 48)]\n","\n","\n","# connect to Pinecone and Open AI\n","index: str = input(\"What is the name of the Pinecone index you wish to upsert to: \") # can change this line just to assinging your index name\n","pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"))\n","if not pc.has_index(index): # you must have an existing pinecone index with the correct dimensions\n","    ValueError(\"Index does not exist.\")\n","\n","pinecone_index = pc.Index(name=index, pool_threads=5)\n","client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n","\n","# format for Pinecone upsert\n","\n","for chunk_batch in refined_chunks:\n","  formatted_batch = []\n","  for record in chunk_batch:\n","    response = client.embeddings.create(\n","        input=record[\"text\"],\n","        model=\"text-embedding-ada-002\" # customize this as you wish\n","    )\n","    embedding = response.data[0].embedding\n","     # print(embedding) uncomment to watch your embeddings get upserted in real-time\n","    formatted_batch.append({\n","            \"id\": record[\"id\"],\n","            \"values\": embedding,\n","            \"metadata\":{\n","            \"order\": record[\"order\"],\n","            \"pre_context\": record[\"pre_context\"],\n","            \"post_context\": record[\"post_context\"],\n","            \"source_description\": record[\"source_description\"],\n","            \"source_url\": record[\"source_url\"],\n","            \"chunk_text\": record[\"text\"]}\n","        })\n","    pinecone_index.upsert(vectors=formatted_batch)\n"],"metadata":{"id":"Th4fprE2dyWk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reading and Querying your non Pinecone Integrated Index"],"metadata":{"id":"rBpo9scVXfPe"}},{"cell_type":"code","source":["res = client.embeddings.create(\n","    input=\"INPUT TEXT HERE\", # change for any vector\n","    model=\"text-embedding-ada-002\"\n",")\n","\n","query_vector = res.data[0].embedding\n","\n","\n","response = pinecone_index.query(\n","    namespace=\"\",\n","    vector=query_vector,\n","    top_k=3,\n","    include_metadata=True,\n","    include_values=True\n",")\n","\n","results: dict = response.to_dict()\n","\n","for match in results[\"matches\"]:\n","    print(match[\"metadata\"][\"chunk_text\"])\n"],"metadata":{"id":"-XZD3b0HXe-3"},"execution_count":null,"outputs":[]}]}